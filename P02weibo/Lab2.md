# 数据爬取实验 {#数据爬取实验 .unnumbered}

## 实验目的

-   了解Python3基础编程语法

-   了解如何安装Python3的第三方库

-   掌握如何用Python实现基于URL的数据爬取

-   掌握如何用Python实现社交媒体网站的数据爬取

-   实现自己修改实验课模板代码实现所需数据爬取

## 实验内容

次实验我们将会了解和熟悉Python3第三方库的安装和使用，并利用Python实现数据爬虫。

## 实验知识点

-   基于URL的数据爬取

-   基于API的数据爬取

-   社交媒体网站的数据爬取

## 实验时长

2学时

## 实验环境

-   **软件环境：**

1）Python3

-   **开发环境与工具：**

1）Sublime Text

## 实验过程

实验任务介绍：本次实验我们将使用Python3实现基于URL、API和用户模拟登录的数据爬取实验，首选请在百度搜索引擎键入自己的名字搜索相关页面，在百度返回的搜索结果中选择一个新闻或门户网站的公开页面URL作为爬取的目的网页。如下示例：

![](media/image1.png){width="4.007692475940507in"
height="1.9322353455818022in"}

**图1-1 百度搜索页面**

注意，本次实验使用的URL样例为知乎的一个公开页面（如图中红框所示），实验过程中请替换为你们自己选择的URL。

1.  

2.  

3.  

4.  

5.  

6.  

## 基于URL的数据爬取

#### 查看目的网页

在Chrome浏览器新建tab页，输入URL页面地址并访问，如本实验示例页面为:
https://www.zhihu.com/question/36390957，打开页面如下图所示：

![](media/image2.png){width="5.085719597550306in"
height="1.7925076552930883in"}

**图1-2 目的网页**

右键点击"检查"查看网页元素，在网页中某个元素点击右键，可以看到源码界面显示了此元素对应的源码片段，从该源码片段中找到元素class或是id属性，如下图所示：

![](media/image3.png){width="5.504314304461943in"
height="1.2923075240594926in"}

**图1-3 查看网页元素**

#### 在Sublime Text中创建开发环境（可使用其他开发环境）

左上角点击"File"-\>"New
file"，将文件名改为"URLspider.py"（文件名可以任取），在Python文件中编写Python爬虫代码。

#### 爬虫代码

代码：

注意：这里用到了URL爬取网页需要的两个第三方库urllib3和certifi，如果系统里头还未安装这两个库，请先完成安装再执行下面的步骤。

在Sublime Text中运行代码（可用快捷键Ctrl+B）,输出：

![](media/image4.png){width="5.430768810148732in"
height="3.0548075240594925in"}

**图1-4 输出的网页内容**

#### 解析和抽取图片并保存到本地

**步骤1** 导入第三方库BeautifulSoup用于解析网页

代码：

**步骤2** 解析网页抽取图片url列表

代码：

输出：

![](media/image5.png){width="5.737544838145232in"
height="1.2597954943132108in"}

**图1-5 输出网页中的图片URL列表**

**步骤3** 导入第三方库wget用于下载图片

代码：

**步骤4**
导入第三方库re用于检测下载图片的URL是否合法，合法则下载图片保存到本地

输出：

![](media/image6.png){width="3.430183727034121in"
height="2.915384951881015in"}

**图1-6 输出下载的图片URL和图片总数**

## 基于用户模拟登录的微博数据爬取

#### 设置调用无界面浏览器Chrome所需的环境

下载chrome：https://www.google.cn/intl/zh-CN/chrome/，查看Chrome的版本，如下图为80.0.3987.132版本的Chrome。

![](media/image7.png){width="4.115277777777778in"
height="1.9152777777777779in"}

**图1-12 下载Chrome并查看版本**

下载ChromeDriver：http://chromedriver.storage.googleapis.com/index.html

因为selenium要用到浏览器的驱动来控制浏览器模拟用户登录微博，下载与你本机Chrome版本相对应的驱动，chromedriver的版本一定要与Chrome的版本一致，不然就不起作用，如本机的版本为80.0.3987.132，下载地址中并无完全吻合的版本，最后下载了80.0.3987.16版本对应的亦可兼容本机，解压压缩包，找到chromedriver.exe复制到chrome的安装目录下Application，将chromedriver.exe文件的路径并加入到电脑的环境变量中去

![](media/image8.png){width="3.8618055555555557in"
height="2.1694444444444443in"}

**图1-13 下载对应版本的ChromeDriver**

#### 查看pc端微博登录页面

打开pc端微博登录页面：http://login.sina.com.cn/，右键点击"检查"查看网页元素，从该源码片段中发现用户名输入框的name为username，密码输入框的name为password，登录按钮为input元素且class为W_btn_a
btn_34px，如下图所示：

![](media/image9.png){width="5.45295384951881in"
height="2.723077427821522in"}

**图1-14 查看pc端微博登录页面相关元素**

#### 在Sublime Text中创建开发环境（可使用其他开发环境）

左上角点击"File"-\>"New
file"，将文件名改为"weiboSpiderDayKWS.py"（文件名可以任取），在Python文件中编写Python爬虫代码。

##### 爬虫代码

**步骤1**调用无界面浏览器Chrome打开pc端微博登录页面

这里用到了模拟用户登录所需的第三方库，请先完成安装再执行下面的步骤。

输出：

![](media/image10.png){width="3.038461286089239in"
height="2.3539643482064743in"}

**图1-15 运行弹出浏览器打开登录页面**

**步骤2** 模拟用户名和密码进行登录

代码：

输出：注意中间输出如下页面，无需验证，等待片刻浏览器会自动验证，如果等待10秒之后浏览器没有自动验证，需要手动进行验证

![](media/image11.png){width="6.023077427821522in"
height="4.255449475065617in"}

**图1-16 用户登录后的验证界面**

![](media/image12.png){width="6.684722222222222in"
height="1.2152777777777777in"}

**图1-17 运行窗口输出信息**

**步骤3**
打开pc端微博登录页面：https://s.weibo.com/，右键点击"检查"查看网页元素，从该源码片段中发现搜索框为input元素且type为text，如下图所示：

![](media/image13.png){width="5.8154713473315836in"
height="1.5307699037620297in"}

**图1-18 查看搜索页面元素**

**步骤3**
模拟打开关键词搜索页面，输入你感兴趣的关键词，模拟输入回车键进行搜索

代码：

输出：

![](media/image14.png){width="5.038461286089239in"
height="3.3687445319335083in"}

**图1-19 模拟键入关键词搜索后的页面**

**步骤4** 解析搜索返回结果的页面，获取微博的具体内容

代码：

![](media/image15.png){width="5.853597987751531in"
height="3.128762029746282in"}

**图1-20 获取微博具体内容的代码**

输出：

![](media/image16.png){width="4.154166666666667in"
height="1.0847222222222221in"}

其他具体内容请参考示例源代码"weiboSpiderDayKWS.py"

## 小组分析实验（课外实践作业）

实践深度优先和宽度优先算法，分析对比两种算法爬取数据的异同点。
